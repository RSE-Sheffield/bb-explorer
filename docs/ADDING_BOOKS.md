* Pre-process the book
   * Convert the pdf to jpg (e.g. using [adobe website](https://www.adobe.com/uk/acrobat/online/pdf-to-jpg.html), easiest to use this in an icognito window to avoid rate limits/forced login)
   * Rename the .jpg files so that they are `<pagenumber>.jpg` with the correct book page number. (The transcriptions have extra and missing pages, and the page number is not always legible to OCR, so this task is largely manual, could potentially be semi-automated with a python script to rename files after N with consecutive numbers and human checking.)
        * At this stage, junk pages (such as covers/blank etc) can be deleted
        * Likewise, remove the contents and any additional tables (often on different sized pages) as these don't need to be transcribed.
    * Transcribe the book using `scripts/gemini_ocr.py` (copy it to working dir and tweak paths)
        * This may require updating the Gemini model version if tagged one has been removed
        * You need to use your own API key, free with a normal google account
        * This script can just be left running, it will transcribe >200 pages/hour, free rate limit should enable atleast 1 book to be completed per day.
        * Sometimes the API will fallover and crash the script (it's not designed to be particularly resilient), restarting the script will continue where it left off.
    * Post-process the split pages into a single `.tex` per section using `scripts/tex_postprocess.py`  (copy it to working dir and tweak paths)
        * This currently requires updating the in/out paths per section.
    * Review each section correcting problems such as:
        * Title not correctly marked as `\section{}`
        * Subsections not correctly marked as `\subsection{}`
        * Words hyphenated over pages being duplicated or not merged.
        * Extended quotes being handled poorly
        * Failure of word wrapping
        * Removing leading/trailing text from a different section
        * Footnotes that run over multiple pages.
        * Other junk at start/end of page, some pages ends with random chars, others begin with the page header which is a contraction of chapter/section.
        * Tables? (most tables have not been manually corrected at this stage), the AI OCR is inconsistent in what format it attempts to transcribe them in.
    * Store the final files in the structure found at the `bb-explorer/bb` directory.
* Extend `index.md` (found at `bb-explorer/bb/index.md` in `langchain` branch) within the LLMs knowledgebase, this should include a summary of each added section, these can be generated by passing full articles to gpt and asking for a concise (e.g. two sentence) summary.
* It may also be necessary to extend `gbooks_dict` within `src/lib/components/chat/Messages/Citations.svelte`, to to map the first 3 characters of the volume category in lowercase (e.g. `Agriculture Vol. 19` -> `agr_19` and volume number to the Google Books URL. The google books URL should be the same as the existing books, with just the ID parameter updated.

